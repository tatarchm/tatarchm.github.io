[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Efficient Learning\n\n\n\n\n\nWhat twenty years of formal education taught me about approaching new information\n\n\n\n\n\nMar 2, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Maxim Tatarchenko",
    "section": "",
    "text": "Google Scholar\n\n\n\n  \n    Histogram-based Deep Learning for Automotive Radar\n    M. Tatarchenko* and K. Rambach*\n    RadarConf\n     (2023)\n    \n      Details\n    \n    \n    \n      \n         PDF\n      \n      \n      \n  \n\n  \n    Fostering Generalization in Single-view 3D Reconstruction by Learning a Hierarchy of Local and Global Shape Priors\n    J. Bechtold, M. Tatarchenko, V. Fischer and T. Brox\n    CVPR\n     (2021)\n    \n      Details\n    \n    \n    \n      \n         PDF\n      \n      \n      \n        \n           Code\n        \n        \n  \n\n  \n    ISOO^V2_DL - semantic instance segmentation of touching and overlapping objects\n    A. Böhm, M. Tatarchenko, and T. Falk\n    ISBI\n     (2019)\n    \n      Details\n    \n    \n    \n      \n         PDF\n      \n      \n      \n  \n\n  \n    Parting with Illusions about Deep Active Learning\n    S. Mittal, M. Tatarchenko, Özgün Çiçek and T. Brox\n    arXiv\n     (2019)\n    \n      Details\n    \n    \n    \n      \n         PDF\n      \n      \n      \n  \n\n  \n    Self-supervised 3d shape and viewpoint estimation from single images\n    O. Mees, M. Tatarchenko, T. Brox and W. Burgard\n    IROS\n     (2019)\n    \n      Details\n    \n    \n    \n      \n         PDF\n      \n      \n      \n  \n\n  \n    Semi-supervised semantic segmentation with high- and low-level consistency\n    S. Mittal, M. Tatarchenko and T. Brox\n    TPAMI\n     (2019)\n    \n      Details\n    \n    \n    \n      \n         PDF\n      \n      \n      \n  \n\n  \n    What do single-view 3d reconstruction networks learn?\n    M. Tatarchenko*, S. R. Richter*, R. Ranftl, Z. Li, V. Koltun, and T. Brox\n    CVPR\n     (2019)\n    \n      Details\n    \n    \n    \n      \n         PDF\n      \n      \n      \n        \n           Code\n        \n        \n  \n\n  \n    Tangent convolutions for dense prediction in 3D\n    M. Tatarchenko, J. Park, V. Koltun, and Q.-Y. Zhou\n    CVPR\n     (2018)\n    \n      Details\n    \n    \n    \n       Project page\n    \n    \n    \n      \n         PDF\n      \n      \n      \n        \n           Code\n        \n        \n  \n\n  \n    Learning to generate chairs, tables and cars with convolutional networks\n    A. Dosovitskiy, J. T. Springenberg, M. Tatarchenko, and T. Brox\n    TPAMI\n     (2017)\n    \n      Details\n    \n    \n    \n      \n         PDF\n      \n      \n      \n  \n\n  \n    Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs\n    M. Tatarchenko, A. Dosovitskiy, and T. Brox\n    ICCV\n     (2017)\n    \n      Details\n    \n    \n    \n       Project page\n    \n    \n    \n      \n         PDF\n      \n      \n      \n        \n           Code\n        \n        \n  \n\n  \n    Multi-view 3d models from single images with a convolutional network\n    M. Tatarchenko, A. Dosovitskiy, and T. Brox\n    ECCV\n     (2016)\n    \n      Details\n    \n    \n    \n       Project page\n    \n    \n    \n      \n         PDF\n      \n      \n      \n        \n           Code\n        \n        \n  \n\n  \n    3D-reconstruction of indoor environments from human activity\n    B. Frank, M. Ruhnke, M. Tatarchenko, and W. Burgard\n    ICRA\n     (2015)\n    \n      Details\n    \n    \n    \n      \n         PDF\n      \n      \n      \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Maxim Tatarchenko",
    "section": "",
    "text": "I am a lead research scientist at Bosch working on 3D deep learning. My main focus at the moment is on company-internal projects but I do occasionally publish papers.\nFor the past couple of years running a robotics school for kids has been my main hobby project. I generally take great pleasure in thinking about teaching and learning, which infrequently results in some blog posts.\nOther than that, I enjoy creating music, reading and writing poetry, making stuff, and blabbering about all of those in my Russian-language TG channel."
  },
  {
    "objectID": "publications/tatarchenko19cvpr_what3d.html",
    "href": "publications/tatarchenko19cvpr_what3d.html",
    "title": "What do single-view 3d reconstruction networks learn?",
    "section": "",
    "text": "Convolutional networks for single-view object reconstruction have shown impressive performance and have become a popular subject of research. All existing techniques are united by the idea of having an encoder-decoder network that performs non-trivial reasoning about the 3D structure of the output space. In this work, we set up two alternative approaches that perform image classification and retrieval respectively. These simple baselines yield better results than state-of-the-art methods, both qualitatively and quantitatively. We show that encoder-decoder methods are statistically indistinguishable from these baselines, thus indicating that the current state of the art in single-view object reconstruction does not actually perform reconstruction but image classification. We identify aspects of popular experimental procedures that elicit this behavior and discuss ways to improve the current state of research."
  },
  {
    "objectID": "publications/tatarchenko19cvpr_what3d.html#abstract",
    "href": "publications/tatarchenko19cvpr_what3d.html#abstract",
    "title": "What do single-view 3d reconstruction networks learn?",
    "section": "",
    "text": "Convolutional networks for single-view object reconstruction have shown impressive performance and have become a popular subject of research. All existing techniques are united by the idea of having an encoder-decoder network that performs non-trivial reasoning about the 3D structure of the output space. In this work, we set up two alternative approaches that perform image classification and retrieval respectively. These simple baselines yield better results than state-of-the-art methods, both qualitatively and quantitatively. We show that encoder-decoder methods are statistically indistinguishable from these baselines, thus indicating that the current state of the art in single-view object reconstruction does not actually perform reconstruction but image classification. We identify aspects of popular experimental procedures that elicit this behavior and discuss ways to improve the current state of research."
  },
  {
    "objectID": "publications/tatarchenko19cvpr_what3d.html#bibtex",
    "href": "publications/tatarchenko19cvpr_what3d.html#bibtex",
    "title": "What do single-view 3d reconstruction networks learn?",
    "section": "Bibtex",
    "text": "Bibtex\n@misc{tatarchenko19cvpr_what3d,\n      title={What do single-view 3d reconstruction networks learn?}, \n      author={Maxim Tatarchenko and Stephan R. Richter and René Ranftl and Zhuwen Li and Vladlen Koltun and Thomas Brox},\n      year={2019},\n      booktitle={CVPR}\n}"
  },
  {
    "objectID": "publications/bechtold21cvpr_generlization.html",
    "href": "publications/bechtold21cvpr_generlization.html",
    "title": "Fostering Generalization in Single-view 3D Reconstruction by Learning a Hierarchy of Local and Global Shape Priors",
    "section": "",
    "text": "Single-view 3D object reconstruction has seen much progress, yet methods still struggle generalizing to novel shapes unseen during training. Common approaches predominantly rely on learned global shape priors and, hence, disregard detailed local observations. In this work, we address this issue by learning a hierarchy of priors at different levels of locality from ground truth input depth maps. We argue that exploiting local priors allows our method to efficiently use input observations, thus improving generalization in visible areas of novel shapes. At the same time, the combination of local and global priors enables meaningful hallucination of unobserved parts resulting in consistent 3D shapes. We show that the hierarchical approach generalizes much better than the global approach. It generalizes not only between different instances of a class but also across classes and to unseen arrangements of objects."
  },
  {
    "objectID": "publications/bechtold21cvpr_generlization.html#abstract",
    "href": "publications/bechtold21cvpr_generlization.html#abstract",
    "title": "Fostering Generalization in Single-view 3D Reconstruction by Learning a Hierarchy of Local and Global Shape Priors",
    "section": "",
    "text": "Single-view 3D object reconstruction has seen much progress, yet methods still struggle generalizing to novel shapes unseen during training. Common approaches predominantly rely on learned global shape priors and, hence, disregard detailed local observations. In this work, we address this issue by learning a hierarchy of priors at different levels of locality from ground truth input depth maps. We argue that exploiting local priors allows our method to efficiently use input observations, thus improving generalization in visible areas of novel shapes. At the same time, the combination of local and global priors enables meaningful hallucination of unobserved parts resulting in consistent 3D shapes. We show that the hierarchical approach generalizes much better than the global approach. It generalizes not only between different instances of a class but also across classes and to unseen arrangements of objects."
  },
  {
    "objectID": "publications/bechtold21cvpr_generlization.html#bibtex",
    "href": "publications/bechtold21cvpr_generlization.html#bibtex",
    "title": "Fostering Generalization in Single-view 3D Reconstruction by Learning a Hierarchy of Local and Global Shape Priors",
    "section": "Bibtex",
    "text": "Bibtex\n@misc{bechtold21cvpr,\n      title={Fostering Generalization in Single-view 3D Reconstruction by Learning a Hierarchy of Local and Global Shape Priors}, \n      author={Jan Bechtold, Maxim Tatarchenko, Volker Fischer and Thomas Brox},\n      year={2021},\n      booktitle={CVPR}\n}"
  },
  {
    "objectID": "publications/dosovitskiy17tpami_chairs.html",
    "href": "publications/dosovitskiy17tpami_chairs.html",
    "title": "Learning to generate chairs, tables and cars with convolutional networks",
    "section": "",
    "text": "We train generative ‘up-convolutional’ neural networks which are able to generate images of objects given object style, viewpoint, and color. We train the networks on rendered 3D models of chairs, tables, and cars. Our experiments show that the networks do not merely learn all images by heart, but rather find a meaningful representation of 3D models allowing them to assess the similarity of different models, interpolate between given views to generate the missing ones, extrapolate views, and invent new objects not present in the training set by recombining training instances, or even two different object classes. Moreover, we show that such generative networks can be used to find correspondences between different objects from the dataset, outperforming existing approaches on this task."
  },
  {
    "objectID": "publications/dosovitskiy17tpami_chairs.html#abstract",
    "href": "publications/dosovitskiy17tpami_chairs.html#abstract",
    "title": "Learning to generate chairs, tables and cars with convolutional networks",
    "section": "",
    "text": "We train generative ‘up-convolutional’ neural networks which are able to generate images of objects given object style, viewpoint, and color. We train the networks on rendered 3D models of chairs, tables, and cars. Our experiments show that the networks do not merely learn all images by heart, but rather find a meaningful representation of 3D models allowing them to assess the similarity of different models, interpolate between given views to generate the missing ones, extrapolate views, and invent new objects not present in the training set by recombining training instances, or even two different object classes. Moreover, we show that such generative networks can be used to find correspondences between different objects from the dataset, outperforming existing approaches on this task."
  },
  {
    "objectID": "publications/dosovitskiy17tpami_chairs.html#bibtex",
    "href": "publications/dosovitskiy17tpami_chairs.html#bibtex",
    "title": "Learning to generate chairs, tables and cars with convolutional networks",
    "section": "Bibtex",
    "text": "Bibtex\n@misc{dosovitskiy17tpami_chairs,\n      title={Learning to generate chairs, tables and cars with convolutional networks}, \n      author={Alexey Dosovitskiy and Jost Tobias Springenberg and Maxim Tatarchenko and Thomas Brox},\n      year={2017},\n      booktitle={TPAMI}\n}"
  },
  {
    "objectID": "publications/tatarchenko18cvpr_tangentconv.html",
    "href": "publications/tatarchenko18cvpr_tangentconv.html",
    "title": "Tangent convolutions for dense prediction in 3D",
    "section": "",
    "text": "We present an approach to semantic scene analysis using deep convolutional networks. Our approach is based on tangent convolutions - a new construction for convolutional networks on 3D data. In contrast to volumetric approaches, our method operates directly on surface geometry. Crucially, the construction is applicable to unstructured point clouds and other noisy real-world data. We show that tangent convolutions can be evaluated efficiently on large-scale point clouds with millions of points. Using tangent convolutions, we design a deep fully-convolutional network for semantic segmentation of 3D point clouds, and apply it to challenging real-world datasets of indoor and outdoor 3D environments. Experimental results show that the presented approach outperforms other recent deep network constructions in detailed analysis of large 3D scenes."
  },
  {
    "objectID": "publications/tatarchenko18cvpr_tangentconv.html#abstract",
    "href": "publications/tatarchenko18cvpr_tangentconv.html#abstract",
    "title": "Tangent convolutions for dense prediction in 3D",
    "section": "",
    "text": "We present an approach to semantic scene analysis using deep convolutional networks. Our approach is based on tangent convolutions - a new construction for convolutional networks on 3D data. In contrast to volumetric approaches, our method operates directly on surface geometry. Crucially, the construction is applicable to unstructured point clouds and other noisy real-world data. We show that tangent convolutions can be evaluated efficiently on large-scale point clouds with millions of points. Using tangent convolutions, we design a deep fully-convolutional network for semantic segmentation of 3D point clouds, and apply it to challenging real-world datasets of indoor and outdoor 3D environments. Experimental results show that the presented approach outperforms other recent deep network constructions in detailed analysis of large 3D scenes."
  },
  {
    "objectID": "publications/tatarchenko18cvpr_tangentconv.html#bibtex",
    "href": "publications/tatarchenko18cvpr_tangentconv.html#bibtex",
    "title": "Tangent convolutions for dense prediction in 3D",
    "section": "Bibtex",
    "text": "Bibtex\n@misc{tatarchenko18cvpr_tangentconv,\n      title={Tangent convolutions for dense prediction in 3D}, \n      author={Maxim Tatarchenko and Jaesik Park and Vladlen Koltun and Qian-Yi Zhou},\n      year={2018},\n      booktitle={CVPR}\n}"
  },
  {
    "objectID": "publications/tatarchenko23radarconf_histdl.html",
    "href": "publications/tatarchenko23radarconf_histdl.html",
    "title": "Histogram-based Deep Learning for Automotive Radar",
    "section": "",
    "text": "There are various automotive applications that rely on correctly interpreting point cloud data recorded with radar sensors. We present a deep learning approach for histogram-based processing of such point clouds. Compared to existing methods, the design of our approach is extremely simple: it boils down to computing a point cloud histogram and passing it through a multi-layer perceptron. Our approach matches and surpasses state-of-the-art approaches on the task of automotive radar object type classification. It is also robust to noise that often corrupts radar measurements, and can deal with missing features of single radar reflections. Finally, the design of our approach makes it more interpretable than existing methods, allowing insightful analysis of its decisions."
  },
  {
    "objectID": "publications/tatarchenko23radarconf_histdl.html#abstract",
    "href": "publications/tatarchenko23radarconf_histdl.html#abstract",
    "title": "Histogram-based Deep Learning for Automotive Radar",
    "section": "",
    "text": "There are various automotive applications that rely on correctly interpreting point cloud data recorded with radar sensors. We present a deep learning approach for histogram-based processing of such point clouds. Compared to existing methods, the design of our approach is extremely simple: it boils down to computing a point cloud histogram and passing it through a multi-layer perceptron. Our approach matches and surpasses state-of-the-art approaches on the task of automotive radar object type classification. It is also robust to noise that often corrupts radar measurements, and can deal with missing features of single radar reflections. Finally, the design of our approach makes it more interpretable than existing methods, allowing insightful analysis of its decisions."
  },
  {
    "objectID": "publications/tatarchenko23radarconf_histdl.html#bibtex",
    "href": "publications/tatarchenko23radarconf_histdl.html#bibtex",
    "title": "Histogram-based Deep Learning for Automotive Radar",
    "section": "Bibtex",
    "text": "Bibtex\n@misc{tatarchenko2023radarconf,\n      title={Histogram-based Deep Learning for Automotive Radar}, \n      author={Maxim Tatarchenko and Kilian Rambach},\n      year={2023},\n      booktitle={RadarConf}\n}"
  },
  {
    "objectID": "publications/tatarchenko17iccv_ogn.html",
    "href": "publications/tatarchenko17iccv_ogn.html",
    "title": "Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs",
    "section": "",
    "text": "We present a deep convolutional decoder architecture that can generate volumetric 3D outputs in a compute- and memory-efficient manner by using an octree representation. The network learns to predict both the structure of the octree, and the occupancy values of individual cells. This makes it a particularly valuable technique for generating 3D shapes. In contrast to standard decoders acting on regular voxel grids, the architecture does not have cubic complexity. This allows representing much higher resolution outputs with a limited memory budget. We demonstrate this in several application domains, including 3D convolutional autoencoders, generation of objects and whole scenes from high-level representations, and shape from a single image."
  },
  {
    "objectID": "publications/tatarchenko17iccv_ogn.html#abstract",
    "href": "publications/tatarchenko17iccv_ogn.html#abstract",
    "title": "Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs",
    "section": "",
    "text": "We present a deep convolutional decoder architecture that can generate volumetric 3D outputs in a compute- and memory-efficient manner by using an octree representation. The network learns to predict both the structure of the octree, and the occupancy values of individual cells. This makes it a particularly valuable technique for generating 3D shapes. In contrast to standard decoders acting on regular voxel grids, the architecture does not have cubic complexity. This allows representing much higher resolution outputs with a limited memory budget. We demonstrate this in several application domains, including 3D convolutional autoencoders, generation of objects and whole scenes from high-level representations, and shape from a single image."
  },
  {
    "objectID": "publications/tatarchenko17iccv_ogn.html#bibtex",
    "href": "publications/tatarchenko17iccv_ogn.html#bibtex",
    "title": "Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs",
    "section": "Bibtex",
    "text": "Bibtex\n@misc{tatarchenko17iccv_ogn,\n      title={Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs}, \n      author={Maxim Tatarchenko and Alexey Dosovitskiy and Thomas Brox},\n      year={2017},\n      booktitle={ICCV}\n}"
  },
  {
    "objectID": "publications/frank15icra_recon.html",
    "href": "publications/frank15icra_recon.html",
    "title": "3D-reconstruction of indoor environments from human activity",
    "section": "",
    "text": "Observing human activities can reveal a lot about the structure of the environment, the objects contained therein and also their functionality. This knowledge, in turn, can be useful for robots interacting with humans or for robots performing mobile manipulation tasks. In this paper, we present an approach to infer the geometric and functional structure of the environment and the position of certain relevant objects in it from human activity. We observe this activity using a full-body motion capture system consisting of a set of inertial measurement units. This is a hard problem since our data suit provides odometry estimates only, which severely drift over time. Therefore, we regard the objects inferred from the activ- ities as landmarks in a graph-based simultaneous localization and mapping problem, which we optimize to obtain accurate estimates about the poses of the objects and the trajectory of the human. In extensive experiments, we demonstrate the effectiveness of the proposed method for the reconstruction of 3D representations. The resulting models not only contain a geometric but also a functional description of the environment and naturally provide a segmentation into individual objects."
  },
  {
    "objectID": "publications/frank15icra_recon.html#abstract",
    "href": "publications/frank15icra_recon.html#abstract",
    "title": "3D-reconstruction of indoor environments from human activity",
    "section": "",
    "text": "Observing human activities can reveal a lot about the structure of the environment, the objects contained therein and also their functionality. This knowledge, in turn, can be useful for robots interacting with humans or for robots performing mobile manipulation tasks. In this paper, we present an approach to infer the geometric and functional structure of the environment and the position of certain relevant objects in it from human activity. We observe this activity using a full-body motion capture system consisting of a set of inertial measurement units. This is a hard problem since our data suit provides odometry estimates only, which severely drift over time. Therefore, we regard the objects inferred from the activ- ities as landmarks in a graph-based simultaneous localization and mapping problem, which we optimize to obtain accurate estimates about the poses of the objects and the trajectory of the human. In extensive experiments, we demonstrate the effectiveness of the proposed method for the reconstruction of 3D representations. The resulting models not only contain a geometric but also a functional description of the environment and naturally provide a segmentation into individual objects."
  },
  {
    "objectID": "publications/frank15icra_recon.html#bibtex",
    "href": "publications/frank15icra_recon.html#bibtex",
    "title": "3D-reconstruction of indoor environments from human activity",
    "section": "Bibtex",
    "text": "Bibtex\n@misc{frank15icra_recon,\n      title={3D-reconstruction of indoor environments from human activity}, \n      author={Barbara Frank, Michael Ruhnke, Maxim Tatarchenko, and Wolfram Burgard},\n      year={2015},\n      booktitle={ICRA}\n}"
  },
  {
    "objectID": "publications/mees19iros_3d.html",
    "href": "publications/mees19iros_3d.html",
    "title": "Self-supervised 3d shape and viewpoint estimation from single images",
    "section": "",
    "text": "We present a convolutional neural network for joint 3D shape prediction and viewpoint estimation from a single input image. During training, our network gets the learning signal from a silhouette of an object in the input image - a form of self-supervision. It does not require ground truth data for 3D shapes and the viewpoints. Because it relies on such a weak form of supervision, our approach can easily be applied to real-world data. We demonstrate that our method produces reasonable qualitative and quantitative results on natural images for both shape estimation and viewpoint prediction. Unlike previous approaches, our method does not require multiple views of the same object instance in the dataset, which significantly expands the applicability in practical robotics scenarios. We showcase it by using the hallucinated shapes to improve the performance on the task of grasping real-world objects both in simulation and with a PR2 robot."
  },
  {
    "objectID": "publications/mees19iros_3d.html#abstract",
    "href": "publications/mees19iros_3d.html#abstract",
    "title": "Self-supervised 3d shape and viewpoint estimation from single images",
    "section": "",
    "text": "We present a convolutional neural network for joint 3D shape prediction and viewpoint estimation from a single input image. During training, our network gets the learning signal from a silhouette of an object in the input image - a form of self-supervision. It does not require ground truth data for 3D shapes and the viewpoints. Because it relies on such a weak form of supervision, our approach can easily be applied to real-world data. We demonstrate that our method produces reasonable qualitative and quantitative results on natural images for both shape estimation and viewpoint prediction. Unlike previous approaches, our method does not require multiple views of the same object instance in the dataset, which significantly expands the applicability in practical robotics scenarios. We showcase it by using the hallucinated shapes to improve the performance on the task of grasping real-world objects both in simulation and with a PR2 robot."
  },
  {
    "objectID": "publications/mees19iros_3d.html#bibtex",
    "href": "publications/mees19iros_3d.html#bibtex",
    "title": "Self-supervised 3d shape and viewpoint estimation from single images",
    "section": "Bibtex",
    "text": "Bibtex\n@misc{mees19iros_3d,\n      title={Self-supervised 3d shape and viewpoint estimation from single images}, \n      author={Oier Mees and Maxim Tatarchenko and Thomas Brox and Wolfram Burgard},\n      year={2019},\n      booktitle={IROS}\n}"
  },
  {
    "objectID": "publications/boehm19isbi_isoodlv2.html",
    "href": "publications/boehm19isbi_isoodlv2.html",
    "title": "ISOO^V2_DL - semantic instance segmentation of touching and overlapping objects",
    "section": "",
    "text": "We present ISOO_DL^V2 - a method for semantic instance segmentation of touching and overlapping objects. We introduce a series of design modifications to the prior framework, including a novel mixed 2D-3D segmentation network and a simplified post-processing procedure which enables segmentation of touching objects without relying on object detection. For the case of overlapping objects where detection is required, we upgrade the bounding box parametrization and allow for smaller reference point distances. All these novelties lead to substantial performance improvements and enable the method to deal with a wider range of challenging practical situations. Additionally, our framework can handle object sub-part segmentation. We evaluate our approach on both real-world and synthetically generated biological datasets and report state-of-the-art performance."
  },
  {
    "objectID": "publications/boehm19isbi_isoodlv2.html#abstract",
    "href": "publications/boehm19isbi_isoodlv2.html#abstract",
    "title": "ISOO^V2_DL - semantic instance segmentation of touching and overlapping objects",
    "section": "",
    "text": "We present ISOO_DL^V2 - a method for semantic instance segmentation of touching and overlapping objects. We introduce a series of design modifications to the prior framework, including a novel mixed 2D-3D segmentation network and a simplified post-processing procedure which enables segmentation of touching objects without relying on object detection. For the case of overlapping objects where detection is required, we upgrade the bounding box parametrization and allow for smaller reference point distances. All these novelties lead to substantial performance improvements and enable the method to deal with a wider range of challenging practical situations. Additionally, our framework can handle object sub-part segmentation. We evaluate our approach on both real-world and synthetically generated biological datasets and report state-of-the-art performance."
  },
  {
    "objectID": "publications/boehm19isbi_isoodlv2.html#bibtex",
    "href": "publications/boehm19isbi_isoodlv2.html#bibtex",
    "title": "ISOO^V2_DL - semantic instance segmentation of touching and overlapping objects",
    "section": "Bibtex",
    "text": "Bibtex\n@misc{boehm19isbi_isoodlv2,\n      title={ISOO^V2_DL - semantic instance segmentation of touching and overlapping objects}, \n      author={Anton Boehm and Maxim Tatarchenko and Thorsten Falk and Thomas Brox},\n      year={2019},\n      booktitle={ISBI}\n}"
  },
  {
    "objectID": "publications/mittal19tpami_semseg.html",
    "href": "publications/mittal19tpami_semseg.html",
    "title": "Semi-supervised semantic segmentation with high- and low-level consistency",
    "section": "",
    "text": "The ability to understand visual information from limited labeled data is an important aspect of machine learning. While image-level classification has been extensively studied in a semi-supervised setting, dense pixel-level classification with limited data has only drawn attention recently. In this work, we propose an approach for semi-supervised semantic segmentation that learns from limited pixel-wise annotated samples while exploiting additional annotation-free images. It uses two network branches that link semi-supervised classification with semi-supervised segmentation including self-training. The dual-branch approach reduces both the low-level and the high-level artifacts typical when training with few labels. The approach attains significant improvement over existing methods, especially when trained with very few labeled samples. On several standard benchmarks - PASCAL VOC 2012, PASCAL-Context, and Cityscapes - the approach achieves new state-of-the-art in semi-supervised learning."
  },
  {
    "objectID": "publications/mittal19tpami_semseg.html#abstract",
    "href": "publications/mittal19tpami_semseg.html#abstract",
    "title": "Semi-supervised semantic segmentation with high- and low-level consistency",
    "section": "",
    "text": "The ability to understand visual information from limited labeled data is an important aspect of machine learning. While image-level classification has been extensively studied in a semi-supervised setting, dense pixel-level classification with limited data has only drawn attention recently. In this work, we propose an approach for semi-supervised semantic segmentation that learns from limited pixel-wise annotated samples while exploiting additional annotation-free images. It uses two network branches that link semi-supervised classification with semi-supervised segmentation including self-training. The dual-branch approach reduces both the low-level and the high-level artifacts typical when training with few labels. The approach attains significant improvement over existing methods, especially when trained with very few labeled samples. On several standard benchmarks - PASCAL VOC 2012, PASCAL-Context, and Cityscapes - the approach achieves new state-of-the-art in semi-supervised learning."
  },
  {
    "objectID": "publications/mittal19tpami_semseg.html#bibtex",
    "href": "publications/mittal19tpami_semseg.html#bibtex",
    "title": "Semi-supervised semantic segmentation with high- and low-level consistency",
    "section": "Bibtex",
    "text": "Bibtex\n@misc{mittal19tpami,\n      title={Semi-supervised semantic segmentation with high- and low-level consistency}, \n      author={Sudhanshu Mittal and Maxim Tatarchenko and Thomas Brox},\n      year={2019},\n      booktitle={TPAMI}\n}"
  },
  {
    "objectID": "publications/mittal19arxiv_illusions.html",
    "href": "publications/mittal19arxiv_illusions.html",
    "title": "Parting with Illusions about Deep Active Learning",
    "section": "",
    "text": "Active learning aims to reduce the high labeling cost involved in training machine learning models on large datasets by efficiently labeling only the most informative samples. Recently, deep active learning has shown success on various tasks. However, the conventional evaluation scheme used for deep active learning is below par. Current methods disregard some apparent parallel work in the closely related fields. Active learning methods are quite sensitive w.r.t. changes in the training procedure like data augmentation. They improve by a large-margin when integrated with semi-supervised learning, but barely perform better than the random baseline. We re-implement various latest active learning approaches for image classification and evaluate them under more realistic settings. We further validate our findings for semantic segmentation. Based on our observations, we realistically assess the current state of the field and propose a more suitable evaluation protocol."
  },
  {
    "objectID": "publications/mittal19arxiv_illusions.html#abstract",
    "href": "publications/mittal19arxiv_illusions.html#abstract",
    "title": "Parting with Illusions about Deep Active Learning",
    "section": "",
    "text": "Active learning aims to reduce the high labeling cost involved in training machine learning models on large datasets by efficiently labeling only the most informative samples. Recently, deep active learning has shown success on various tasks. However, the conventional evaluation scheme used for deep active learning is below par. Current methods disregard some apparent parallel work in the closely related fields. Active learning methods are quite sensitive w.r.t. changes in the training procedure like data augmentation. They improve by a large-margin when integrated with semi-supervised learning, but barely perform better than the random baseline. We re-implement various latest active learning approaches for image classification and evaluate them under more realistic settings. We further validate our findings for semantic segmentation. Based on our observations, we realistically assess the current state of the field and propose a more suitable evaluation protocol."
  },
  {
    "objectID": "publications/mittal19arxiv_illusions.html#bibtex",
    "href": "publications/mittal19arxiv_illusions.html#bibtex",
    "title": "Parting with Illusions about Deep Active Learning",
    "section": "Bibtex",
    "text": "Bibtex\n@misc{mittal19arxiv_illusions,\n      title={Parting with Illusions about Deep Active Learning}, \n      author={Sudhanshu Mittal and Maxim Tatarchenko and Özgün Çiçek and Thomas Brox},\n      year={2019},\n      booktitle={arXiv}\n}"
  },
  {
    "objectID": "publications/tatarchenko16eccv_mv3d.html",
    "href": "publications/tatarchenko16eccv_mv3d.html",
    "title": "Multi-view 3d models from single images with a convolutional network",
    "section": "",
    "text": "We present a convolutional network capable of inferring a 3D representation of a previously unseen object given a single image of this object. Concretely, the network can predict an RGB image and a depth map of the object as seen from an arbitrary view. Several of these depth maps fused together give a full point cloud of the object. The point cloud can in turn be transformed into a surface mesh. The network is trained on renderings of synthetic 3D models of cars and chairs. It successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars."
  },
  {
    "objectID": "publications/tatarchenko16eccv_mv3d.html#abstract",
    "href": "publications/tatarchenko16eccv_mv3d.html#abstract",
    "title": "Multi-view 3d models from single images with a convolutional network",
    "section": "",
    "text": "We present a convolutional network capable of inferring a 3D representation of a previously unseen object given a single image of this object. Concretely, the network can predict an RGB image and a depth map of the object as seen from an arbitrary view. Several of these depth maps fused together give a full point cloud of the object. The point cloud can in turn be transformed into a surface mesh. The network is trained on renderings of synthetic 3D models of cars and chairs. It successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars."
  },
  {
    "objectID": "publications/tatarchenko16eccv_mv3d.html#bibtex",
    "href": "publications/tatarchenko16eccv_mv3d.html#bibtex",
    "title": "Multi-view 3d models from single images with a convolutional network",
    "section": "Bibtex",
    "text": "Bibtex\n@misc{tatarchenko16eccv_mv3d,\n      title={Multi-view 3d models from single images with a convolutional network}, \n      author={Maxim Tatarchenko and Alexey Dosovitskiy and Thomas Brox},\n      year={2016},\n      booktitle={ECCV}\n}"
  },
  {
    "objectID": "blog/2019_03_02_efficient_learning.html",
    "href": "blog/2019_03_02_efficient_learning.html",
    "title": "Efficient Learning",
    "section": "",
    "text": "For the last twenty years, my main day-to-day activity has been learning. This may seem like a lot at first glance but such is the world we live in: kindergarten, school, undergrad, graduate school… time adds up. And while those have been amazing years, I now realize that some of them could have been spent much more efficiently if I had known certain basic principles of learning. In this post I want to talk about three such principles: remembering the world is not random, developing appropriate intuition and finding the right abstractions.\n\n\nRemember the world is not random.\nBack in high school, I had a class called “Algebra and the basics of mathematical analysis”. There I learned that in order to find the extreme values of a function, you need to compute its derivative, then solve some equation, then check for some conditions… In fact, there was a 10-step algorithm which I successfully mastered. What I could not realize at the time was why on earth would anyone want to do this?\nThe question “why?” turns out to be essential. It almost always gives you the right perspective for understanding things. The world around us is not random. People don’t start developing complex theories to put them into textbooks and make the school kids’ lives miserable. In the vast majority of cases, there is some motivation behind the process. Unfortunately, as theories make their way through the years, the initial motivation often fades away leaving us with dogmatic algorithms like that of finding a function’s extremal values.\nWhat I should have thought about back then are questions like “Why do we need functions in the first place?” Well, the answer is exactly because the world is not random and many things depend on many other things. The number of likes you get for a picture depends on the time of day you post it on Instagram. Functions help us reveal those dependencies, understand them. “Why do I need to find the extremal values of functions?” Well, because this is where the most interesting things usually happen. You want to get as many likes as possible for your picture — an event described by the maximum value of the corresponding fucntion.\nClearly, the “why”-perspective is useful not only in mathematics. When studying history, forcing yourself to think about the “why’s” of events turns a series of random wars and revolutions into a meaningful logical flow. When learning to play guitar, you can blindly memorize songs for years. But analyzing why the chords in a song sound good together in the first place, can give you a whole new level of understanding.\n\n\nDevelop intuition.\nDuring my bachelor studies I had to take a lot of math courses. One of those was about Fourier analysis. Two hours a week for about three months in a row the professor would bombard us with all kinds of definitions, lemmas and theorems filling the blackboard with kilometers of formulas. I’m pretty sure it was all quality content. Problem is, now I don’t remember a single bit of it — he lost my attention after the first 10 minutes of the introductory lecture.\nIt took me years to understand that this sort of information is completely useless without the intuition. Humans (well, at least most of us) don’t think in formal definitions, abstract concepts or mathematical formulas. We need to imagine stuff. We need to relate what we learn to something we already know — to familiar things from everyday life. Of course, it’s amazing if the teacher himself takes care of developing the right intuition in his students’ minds. But if not, there is nothing else left but to help yourself. Richard Feynman was a great master of this skill. Here’s how he describes communication with fellow physicists in his (hands down hilarious) book.\n\nAt all these places everybody working in physics would tell me what they were doing and I’d discuss it with them. They would tell me the general problem they were working on, and would begin to write a bunch of equations.\n“Wait a minute,” I would say. “Is there a particular example of this general problem?”\n“Why yes; of course.”\n“Good. Give me one example.” That was for me: I can’t understand anything in general unless I’m carrying along in my mind a specific example and watching it go. Some people think in the beginning that I’m kind of slow and I don’t understand the problem, because I ask a lot of these “dumb” questions: “Is a cathode plus or minus? Is an an-ion this way, or that way?”\nBut later, when the guy’s in the middle of a bunch of equations, he’ll say something and I’ll say, “Wait a minute! There’s an error! That can’t be right!”\nThe guy looks at his equations, and sure enough, after a while, he finds the mistake and wonders, “How the hell did this guy, who hardly understood at the beginning, find that mistake in the mess of all these equations?”\nHe thinks I’m following the steps mathematically, but that’s not what I’m doing. I have the specific, physical example of what he’s trying to analyze, and I know from instinct and experience the properties of the thing. So when the equation says it should behave so-and-so, and I know that’s the wrong way around, I jump up and say, “Wait! There’s a mistake!”\nI had a scheme, which I still use today when somebody is explaining something that I’m trying to understand: I keep making up examples. For instance, the mathematicians would come in with a terrific theorem, and they’re all excited. As they’re telling me the conditions of the theorem, I construct something which fits all the conditions. You know, you have a set (one ball) — disjoint (two balls). Then the balls turn colors, grow hairs, or whatever, in my head as they put more conditions on. Finally they state the theorem, which is some dumb thing about the ball which isn’t true for my hairy green ball thing, so I say, “False!”\n\nThe guy’s opinion is definitely trustworthy. In the end, he received a Nobel prize in physics so he must have known what he was saying. In this passage we can spot a useful detail: the main tool for developing intuition is analogy. When learning something new, you can try to imagine something you already understand which has analogous properties.\nImagine you stumble upon a statement like “in an electric circuit, the relationship between the resistance and the area of the cross section of a wire is inversely proportional.” This may look pretty intimidating at first glance. But as soon as you substitute the electric circuit with a water pipe in your mind, and imagine that electric current is water flow, the effect becomes obvious. The thicker you pipe is, the more water it can let through — thus the lower the resistance.\nThe analogy I was missing during the Fourier analysis course was that essentially a Fourier transform of a function is a coordinate system — similar to that used in your GPS navigator. The latitude and the longitude of your favourite pizza place tell you that in order to reach it, you need to go this much north and then this much west. Turns out, it is possible to define a coordinate system for functions. Take this much of one function, add this much of another, and voila — you get your desired function.\nOf course, one needs to be careful with analogies. They are never perfect and have their limitations. But overall it’s a great way to develop some initial understanding when learning new things.\n\n\nFind the right abstraction.\nThe world around us is complex. Extremely complex. So complex in fact, that our poor brains cannot possibly process all the information that reaches us from outside. Good news is, in most situations we don’t need all the information. We can (and do) make sense of what we see, hear or feel by assigning it to some simple categories. That is, by thinking in more abstract and general terms rather than attending to the finest details.\nRobert Sapolsky in his lecture on behavioural biology gives an example with colors. Objectively, there is no such thing as red, green or blue. Instead, there is a continuous spectrum of wave lengths which can be registered by the human eye. We just conventionally agreed to cut this spectrum into seven bins and gave those bins certain names. For many practical purposes it’s good enough. When you are driving a car, it’s sufficient to distinguish red from green on a street light — the exact wave length does not matter. In a child’s picture the sky is blue and the grass is green, it can be any kind of blue or green depending on what the pencil producer happened to put into the box. (Well, there are some exceptions of course. If you are shopping for new shoes, the difference between ruby red and coral red may be pretty huge.)\nUsually, we can immensely simplify the process of information consumption by identifying the right abstractions. What “the right” is, clearly depends on the task being solved. As a rule of thumb, let’s stick with the principle often attributed to Albert Einstein: “Everything should be made as simple as possible, but not simpler.”\nThree years ago I started a PhD in computer science. As a researcher, I naturally learn most of my time. When reading scientific papers, I learn new ideas developed by other people. When skimming through programming manuals, I learn about new tools and ways to use them. When running my own experiments, I learn something about the research problem I’m solving. It is so easy and tempting to get absorbed in details. Sometimes I start designing a new algorithm and run some initial experiments. I realize there is a parameter that might slightly affect the performance so I start tweaking this parameter. Indeed, there is some relation between it and the final result but it somehow looks strange. So I investigate more… Three weeks later I find myself struggling over a 1% performance gain which is completely unimportant and does not even help answer the initial question.\nSuch situations can hamper your progress quite a bit, and the only way of avoiding them is to constantly remind yourself about maintaining the right abstractions. Again, this does not mean that details are not important — the devil is still in them. But sometimes you don’t want to let him speak.\n\nThe ideas I’ve described are no magic shortcuts. Even if you use all of them, there is no way around routine tasks. To master Taekwon-Do, you need to develop your body, no matter whether you understand the basic theory or not. To calculate integrals, you have to go through actually calculating maybe a hundred of them, not just figure out how or why this is done. As long as we don’t live in The Matrix, there is no way of upgrading our brain’s firmware to support some desired skill. Still, being aware of the basic principles when doing some routine practice, may make it more meaningful and exciting.\nAt this point you may be willing to strike me with my own weapon by asking: “Why do we need all of this?” Let me leave you with a bit of general motivation. Genetic evolution is about the survival of the fittest. In an ever-changing environment, only species whose set of genes has adjusted successfully, stand a chance. However, such environmental changes can last millions of years, so normally there is quite some time to adapt. On the contrary, the society we live in changes rapidly. Individual skills, even entire occupations can become obsolete within a time span of several years. Therefore, the key to success in this social evolution is being able to quickly adapt yourself. That is, master new skills, understand new concepts and adjust your behaviour accordingly. The more efficient you are in all this, the higher your chances to survive. What can be more motivating?"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "tatarchm@gmail.com | +4915788159993"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "EDUCATION",
    "text": "EDUCATION\nAlbert-Ludwigs-Universität Freiburg Jan. 2016 — Feb. 2020  PhD (summa cum laude) in Computer Science  Computer Vision Lab, advisor Prof. Dr.-Ing. Thomas Brox  Final grade 0.0, with distinction\nAlbert-Ludwigs-Universität Freiburg Oct. 2012 — Mar. 2013  Master in Computer Science Apr. 2014 — Dec. 2015 Final grade 0.0, with distinction\n“MATI” - K. I. Tsiolkovsky Russian State Technological University Oct. 2012 — Mar. 2013  Bachelor in Applied Mathematics and Informatics  Final grade 4,8, with distinction"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "PROFESSIONAL EXPERIENCE",
    "text": "PROFESSIONAL EXPERIENCE\nBosch, Renningen, Germany Nov. 2023 — now  Lead Research Scientist  AI Research Department\nBosch, Renningen, Germany May. 2020 — Oct. 2023  Research Scientist  AI Research Department\nAlbert-Ludwigs-Universität Freiburg, Germany Jan. 2016 — Feb. 2020  Research Assistant  Computer Vision Lab\nIntel Labs, Santa Clara, USA May. 2017 — Nov. 2017  Research Intern  Intelligent Systems Lab, advisor Dr. Vladlen Koltun\nAlbert-Ludwigs-Universität Freiburg, Germany Jun. 2014 — Dec. 2015  Student Research Assistant  Autonomous Intelligent Systems Lab\nGPSCOM, Moscow, Russia Dec. 2011 — Apr. 2014  Software Engineer\nCrechet corp., Moscow, Russia Jun. 2011 — Dec. 2011  Software Developer"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "PUBLICATIONS",
    "text": "PUBLICATIONS\nGoogle scholar citations: 2650.\nNot including publications in Russian prior to 2015.\n\nReferred papers\n\nJ. Kälble, S. Wirges, M. Tatarchenko and E. Ilg “Accurate Training Data for Occupancy Map Prediction in Automated Driving using Evidence Theory” In CVPR, 2024\nM. Tatarchenko, K. Rambach “Histogram-based Deep Learning for Automotive Radar.” In RadarConf, 2023\nJ. Bechtold, M. Tatarchenko, V. Fischer and T. Brox “Fostering Generalization in Single-view 3D Reconstruction by Learning a Hierarchy of Local and Global Shape Priors.” In CVPR, 2021\nS. Mittal, M. Tatarchenko and T. Brox. “Semi-supervised semantic segmentation with high- and low-level consistency.” In TPAMI, 2019\nO. Mees, M. Tatarchenko, T. Brox and W. Burgard. “Self-supervised 3d shape and viewpoint estimation from single images.” In IROS, 2019\nM. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox. “What do single-view 3d reconstruction networks learn?” In CVPR, 2019\nA. Böhm, M. Tatarchenko, and T. Falk. “ISOO^V2_DL - semantic instance segmentation of touching and overlapping objects.” In ISBI, 2019\nM. Tatarchenko, J. Park, V. Koltun, and Q.-Y. Zhou. “Tangent convolutions for dense prediction in 3d.” In CVPR, 2018 (Selected for spotlight oral)\nA. Dosovitskiy, J. T. Springenberg, M. Tatarchenko, and T. Brox. “Learning to generate chairs, tables and cars with convolutional networks.” TPAMI, Apr 2017\nM. Tatarchenko, A. Dosovitskiy, and T. Brox. “Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs.” In ICCV, 2017\nM. Tatarchenko, A. Dosovitskiy, and T. Brox. “Multi-view 3d models from single images with a convolutional network.” In ECCV, 2016 (Selected for spotlight oral)\nB. Frank, M. Ruhnke, M. Tatarchenko, and W. Burgard. “3d-reconstruction of indoor environments from human activity.” In ICRA, 2015\n\n\n\nPreprints\n\nS. Mittal, M. Tatarchenko, Özgün Çiçek and T. Brox. “Parting with Illusions about Deep Active Learning.” In arXiv:1912.05361, 2019\n\n\n\nTheses\n\n“Scalable 3D deep learning: methods and applications”, PhD thesis, 2020\n“Generating unseen views of objects with convolutional networks”, Master’s thesis, 2015"
  },
  {
    "objectID": "cv.html#professional-services",
    "href": "cv.html#professional-services",
    "title": "Curriculum Vitae",
    "section": "PROFESSIONAL SERVICES",
    "text": "PROFESSIONAL SERVICES\nReviewer for IROS’18, ICCV’18, CVPR’18, CVPR’19 (outstanding reviewer), TPAMI’19, CVPR’20, IJCV’20, CVPR’21 (outstanding reviewer), RA-L’21, TPAMI’21, TPAMI’22, CVPR’23, CVPR’24"
  },
  {
    "objectID": "cv.html#awards",
    "href": "cv.html#awards",
    "title": "Curriculum Vitae",
    "section": "AWARDS",
    "text": "AWARDS\nVDI-Förderpreis 2016  Sponsorship award of the Association of German Engineers  Awarded for the master’s thesis"
  },
  {
    "objectID": "cv.html#media-coverage",
    "href": "cv.html#media-coverage",
    "title": "Curriculum Vitae",
    "section": "MEDIA COVERAGE",
    "text": "MEDIA COVERAGE\n3sat: Scobel 2016  TV program about AI  Mentioned the work “Multi-view 3D models from single images with CNNs”"
  },
  {
    "objectID": "cv.html#patents",
    "href": "cv.html#patents",
    "title": "Curriculum Vitae",
    "section": "PATENTS",
    "text": "PATENTS\nComputer-implemented method and system for reconstructing an object captured by an imaging sensor, and training method 2022  DE patent “DE102021202711 A1””  J. Bechtold, T. Brox, V. Fischer and M. Tatarchenko\nTangent convolutions for 3D data 2019  US patent “US2019042883 AA”  J. Park, V. Koltun, M. Tatarchenko and Q.-Y. Zhou"
  },
  {
    "objectID": "cv.html#language-skills",
    "href": "cv.html#language-skills",
    "title": "Curriculum Vitae",
    "section": "LANGUAGE SKILLS",
    "text": "LANGUAGE SKILLS\nRussian (mother tongue), English (advanced), German (advanced)"
  },
  {
    "objectID": "cv.html#teaching-experience",
    "href": "cv.html#teaching-experience",
    "title": "Curriculum Vitae",
    "section": "TEACHING EXPERIENCE",
    "text": "TEACHING EXPERIENCE\n\nPhD student supervision\nJonas Kälble Apr. 2023 — now  Image-based occupancy estimation  University of Saarland and Bosch\nMelis Öcal Sep. 2022 — now  Generative modelling for 3D reconstruction  University of Amsterdam and Bosch Delta Lab 2\nRonny Xavier Velastegui Sandoval Oct. 2022 — now  3D semantic segmentation  University of Amsterdam and Bosch Delta Lab 2\nJan Bechtold Apr. 2021 — Mar. 2023  Single-view 3D reconstruction  University of Freiburg and Bosch\n\n\nMaster/bachelor/intern supervision\nYuchen Tao Oct. 2021 — Apr. 2022  Point cloud completion via direct measurement integration  Master intern at BCAI\nOlesya Tsapenko Mar. 2019 — Sep. 2019  Point cloud colorization using sparse convolutions  Master’s thesis\nJan Bechtold Jun. 2018 — Dec. 2018  3D object detection using tangent convolutions  Master’s thesis\nLukas Wiens Dec. 2017 — Mar. 2018  Implementierung der Octree Generating Networks Deep Learning Architektur in Tensorflow  Bachelor’s thesis\nSudhanshu Mittal Mar. 2017 — Nov. 2017  Semi-supervised learning for real-world object recognition using adversarial autoencoders  Master’s thesis\nVladislav Tananaev Mar. 2017 — Jun. 2017  Semantic segmentation in point clouds with deep networks  Master’s thesis\n\n\nUniversity courses\nOptimization (in German) WS 2019 — 2020  Lecture  Teaching assistant\nStatistical pattern recognition 2018 — 2019  Lecture, selected classes  Lecturer\nComputer vision 2018  Lecture, selected classes  Lecturer\nDeep learning for biomedical image analysis 2016 — 2019  Seminar  Supervisor\nCurrent works in computer vision 2016 — 2019  Seminar  Supervisor\nDeep learning SS 2016  Lab course  Co-organizer and supervisor\nParking space detection SS 2015  Lab course  Co-organizer"
  },
  {
    "objectID": "cv.html#selected-talks",
    "href": "cv.html#selected-talks",
    "title": "Curriculum Vitae",
    "section": "SELECTED TALKS",
    "text": "SELECTED TALKS\nNot including internal company/lab talks, not including talks prior to 2016.\n3D deep learning: methods and applications Jul. 2020  PhD defence, Freiburg, Germany\n3D deep learning: methods and applications Dec. 2019  5th Christmas Colloquium on Computer Vision, Yandex, Moscow\nWhat do single-view 3d reconstruction networks learn? Jul. 2019  Dynamic Vision workshop, CVPR, Long Beach\nProblems of single-image 3d reconstruction Sep. 2018  Intel Network on Intelligent Systems Workshop, Munich\nDeep learning in computer vision and its applications to 3D data Jun. 2018  Optics Colloquium, University of Freiburg\nMulti-view 3D models from single images with a convolutional network Dec. 2016  2nd Christmas Colloquium on Computer Vision, Skoltech, Moscow\nMulti-view 3D models from single images with a convolutional network Oct. 2016  ECCV, Amsterdam\nGraduation speech Jul. 2016  Graduation ceremony, University of Freiburg"
  },
  {
    "objectID": "cv.html#volunteering-activities",
    "href": "cv.html#volunteering-activities",
    "title": "Curriculum Vitae",
    "section": "VOLUNTEERING ACTIVITIES",
    "text": "VOLUNTEERING ACTIVITIES\nRobotics workshop for Ukrainian kids May. 2022 — now  Organizer\nYouth hackathon Freiburg Nov. 2019  Mentor"
  }
]